# Project SEAL on MLX: Phase 1 - Synthetic Data Generation
### Date: June 23, 2025

Objective: To replicate the initial data generation phase of the SEAL (Self-Adapting Language Models) paper on a macOS environment with Apple Silicon, using the MLX framework.

## 1. Project Goal
The primary goal of this project is to replicate and extend the findings of the SEAL paper. The first critical step, which this document covers, is adapting the synthetic data generation pipeline. The original implementation relies on a VLLM server and SLURM for execution on Linux-based GPU clusters. Our objective was to modify this pipeline to run locally on a MacBook Pro, leveraging its Apple Silicon GPU via the MLX library.

## 2. Methodology & Execution
We successfully adapted the process by replacing the core components with MLX-equivalents and creating new scripts for local execution.

### 2.1. Environment Setup
A Conda environment (seal_mlx) was established with Python 3.12. The key libraries installed were:
 * mlx and mlx-lm: For running language models on Apple Silicon.
 * transformers: For components related to model and tokenizer handling.
 * numpy, requests, tqdm: Standard utility libraries.

### 2.2. The Data Generation Script (make_squad_data_mlx.py)
The original make_squad_data.py was designed to query a remote VLLM server endpoint. We created a new script, make_squad_data_mlx.py, which performs the generation locally.
Core Logic:
 * Model Loading: Instead of calling an API, it uses mlx_lm.load() to load a Hugging Face model repository directly into the Mac's unified memory.
 * Local Generation: It iterates through articles from the SQuAD dataset and uses mlx_lm.generate() to generate k completions for each article.
 * Sampling Control: We implemented sampling control using mlx_lm.sample_utils.make_sampler to manage temperature and top_p parameters effectively.
 * Organized Output: The script saves the results and a metadata file into a new, separate directory (mlx_experiments) to keep our findings distinct from the original repository's files.

### 2.3. The Execution Script (make_squad_data_mlx.sh)
We created a new shell script to manage the execution locally.
Core Logic:
 * No SLURM/VLLM: All SLURM directives and VLLM server startup logic were removed.
 * Configuration: Provides a clean, user-editable section for setting parameters like the MODEL_ID, file paths, and generation settings (k, temperature, etc.).
 * Robust Argument Passing: Uses a bash array (ARGS) to safely build the command and pass arguments (including optional flags like --instruct_model) to the Python script.

### 2.4. Execution & Performance Findings
 * Model: mlx-community/Qwen1.5-7B-Chat-MLX-4bit
 * Task: Generated 5 completions (k=5) for each of 50 articles (n=50), with max_tokens=1024.
 * Total Completions: 250
 * Total Time: ~4851 seconds (~81 minutes)
 * Average Time per Article: ~97 seconds

## 3. Analysis of Results
The primary result is the generated JSON file: knowledge-incorporation/mlx_experiments/data/synthetic_data/train/squad_train_mlx_generated.json.

### 3.1. Deep Dive: The "Antarctica" Sample
Let's analyze the provided sample record to understand the output's structure and significance.
  {
    "title": "Antarctica",
    "context": "Although coal, hydrocarbons, iron ore, platinum, copper, chromium, nickel, gold and other minerals have been found...",
    "questions": [
      { "question": "...", "answer": "..." }
    ],
    "completions": [
      "<think>...</think>\n\n**Implications Derived from the Passage:**\n\n1. **Environmental Protections Limit Resource Exploitation...**",
      "<think>...</think>\n\n**Implications Derived from the Passage on Antarctica:**\n\n1. **Limited Economic Exploitation of Mineral Resources...**",
      // ... 3 more completion strings ...
    ],
    "prompt": "<|im_start|>system\nYou are an assistant tasked with analyzing the provided passage...<|im_end|>\n<|im_start|>assistant\n"
  }

| Field | Description |
|---|---|
| title | The title of the source article from the SQuAD dataset. |
| context | The source text paragraph from the SQuAD article that was fed to the language model. |
| questions | The original question-answer pairs from the SQuAD dataset. Note: This data is carried over from the source file but is not used in this specific data generation step. It's simply preserved. |
| prompt | The exact, complete prompt that was sent to the MLX model for generation. This includes the system message, the user message (containing the title and context), and the final prompt for the assistant to begin its response. |
| completions | This is a list containing 5 strings (k=5). Each string is a unique, complete response generated by the model for the given prompt. |

### 3.2. The Significance of the completions Field
The completions array is the entire point of this phase. In the context of SEAL, this generated data serves as the target knowledge for self-updating.
 * Source of Truth: Each completion represents a set of facts, implications, or rewrites related to the original context.
 * Training Material: In the subsequent phases of SEAL (like Test-Time Training), the model will be tasked with learning the information contained in one of these completions.
 * Goal of Self-Editing: The ultimate goal is to train a model that, when faced with new text, can generate its own fine-tuning data (similar to these completions) to update its internal knowledge base without needing a human to create that data manually. This completions array is our first, human-supervised step to bootstrap that process.

### 3.3. Understanding the <think> Blocks
Inside each completion, we see a block of text enclosed in <think>...</think> tags. This is a direct result of a prompting technique called Chain-of-Thought (CoT) or a "Scratchpad".
 * Purpose: The prompt implicitly asks the model to "think step-by-step" before providing the final, formatted list of implications.
 * How it Works: The model first writes out its internal monologue: it deconstructs the request, analyzes the passage, brainstorms potential implications, and refines its ideas.
 * Benefit: This process of forcing the model to "show its work" dramatically improves the quality, accuracy, and relevance of the final output. It prevents the model from giving a rushed, superficial answer.
 * Stochastic Nature: You can see that the thought process is slightly different in each of the 5 completions, even though the final implications are similar. This reflects the probabilistic nature of language models and is why generating multiple completions (k > 1) is valuable for finding the best possible training data.
